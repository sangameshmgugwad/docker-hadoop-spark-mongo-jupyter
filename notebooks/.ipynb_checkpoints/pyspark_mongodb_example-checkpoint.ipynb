{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySpark with MongoDB Integration Example\n",
    "\n",
    "This notebook demonstrates:\n",
    "1. Connecting to Spark cluster\n",
    "2. Using PySpark for data processing\n",
    "3. Using Spark ML for machine learning\n",
    "4. Reading/Writing data to MongoDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Initialize Spark Session with MongoDB Support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "import pymongo\n",
    "\n",
    "# Stop any existing Spark session first\n",
    "try:\n",
    "    spark.stop()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Create Spark Session with MongoDB connector\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PySpark-MongoDB-Example\") \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .config(\"spark.mongodb.input.uri\", \"mongodb://admin:admin@mongodb-dba:27017/test.collection?authSource=admin\") \\\n",
    "    .config(\"spark.mongodb.output.uri\", \"mongodb://admin:admin@mongodb-dba:27017/test.collection?authSource=admin\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.mongodb.spark:mongo-spark-connector_2.12:3.0.1\") \\\n",
    "    .config(\"spark.executor.memory\", \"1g\") \\\n",
    "    .config(\"spark.driver.memory\", \"1g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(f\"Spark Version: {spark.version}\")\n",
    "print(f\"Spark Master: {spark.sparkContext.master}\")\n",
    "print(f\"Application ID: {spark.sparkContext.applicationId}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternative: Use Existing Spark Session (Simpler)\n",
    "\n",
    "If you just want to use the pre-configured Spark session without MongoDB connector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the pre-existing spark variable (already configured in this notebook image)\n",
    "# This is simpler and works immediately\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Get or create spark session (reuses existing one if available)\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SimplePySparkExample\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(f\"Spark Version: {spark.version}\")\n",
    "print(f\"Spark Master: {spark.sparkContext.master}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create Sample Data and Basic Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample data\n",
    "data = [\n",
    "    (1, \"John\", 28, 50000, \"Engineering\"),\n",
    "    (2, \"Jane\", 32, 75000, \"Marketing\"),\n",
    "    (3, \"Bob\", 45, 95000, \"Engineering\"),\n",
    "    (4, \"Alice\", 29, 62000, \"Sales\"),\n",
    "    (5, \"Charlie\", 38, 88000, \"Engineering\")\n",
    "]\n",
    "\n",
    "columns = [\"id\", \"name\", \"age\", \"salary\", \"department\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "print(\"Sample DataFrame:\")\n",
    "df.show()\n",
    "\n",
    "print(\"\\nDataFrame Schema:\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Basic DataFrame Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter employees with salary > 60000\n",
    "high_earners = df.filter(df.salary > 60000)\n",
    "print(\"Employees with salary > 60000:\")\n",
    "high_earners.show()\n",
    "\n",
    "# Group by department and calculate average salary\n",
    "print(\"\\nAverage salary by department:\")\n",
    "df.groupBy(\"department\").avg(\"salary\", \"age\").show()\n",
    "\n",
    "# Select specific columns\n",
    "print(\"\\nNames and Departments:\")\n",
    "df.select(\"name\", \"department\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Write Data to MongoDB (if MongoDB connector is configured)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: This requires MongoDB Spark Connector JARs to be loaded\n",
    "# If you get an error, use PyMongo instead (see section 6)\n",
    "\n",
    "try:\n",
    "    df.write.format(\"mongo\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .option(\"uri\", \"mongodb://admin:admin@mongodb-dba:27017/test.employees?authSource=admin\") \\\n",
    "        .save()\n",
    "    print(\"Data written to MongoDB successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error writing to MongoDB via Spark: {e}\")\n",
    "    print(\"\\nTip: Use PyMongo for direct MongoDB operations (see section 6)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Read Data from MongoDB (if connector is configured)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    df_from_mongo = spark.read.format(\"mongo\") \\\n",
    "        .option(\"uri\", \"mongodb://admin:admin@mongodb-dba:27017/test.employees?authSource=admin\") \\\n",
    "        .load()\n",
    "    \n",
    "    print(\"Data read from MongoDB:\")\n",
    "    df_from_mongo.show()\n",
    "except Exception as e:\n",
    "    print(f\"Error reading from MongoDB via Spark: {e}\")\n",
    "    print(\"\\nTip: Use PyMongo for direct MongoDB operations (see section 6)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Spark ML - Linear Regression Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "# Create sample data for regression\n",
    "ml_data = [\n",
    "    (1, 2.0, 3.0, 100.0),\n",
    "    (2, 3.0, 4.0, 150.0),\n",
    "    (3, 4.0, 5.0, 200.0),\n",
    "    (4, 5.0, 6.0, 250.0),\n",
    "    (5, 6.0, 7.0, 300.0),\n",
    "    (6, 7.0, 8.0, 350.0),\n",
    "    (7, 8.0, 9.0, 400.0),\n",
    "]\n",
    "\n",
    "ml_df = spark.createDataFrame(ml_data, [\"id\", \"feature1\", \"feature2\", \"label\"])\n",
    "\n",
    "# Prepare features\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"feature1\", \"feature2\"],\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "ml_df_assembled = assembler.transform(ml_df)\n",
    "\n",
    "print(\"Data with assembled features:\")\n",
    "ml_df_assembled.select(\"features\", \"label\").show()\n",
    "\n",
    "# Split data\n",
    "train_data, test_data = ml_df_assembled.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Train model\n",
    "lr = LinearRegression(featuresCol=\"features\", labelCol=\"label\")\n",
    "lr_model = lr.fit(train_data)\n",
    "\n",
    "# Make predictions\n",
    "predictions = lr_model.transform(test_data)\n",
    "\n",
    "print(\"\\nLinear Regression Predictions:\")\n",
    "predictions.select(\"features\", \"label\", \"prediction\").show()\n",
    "\n",
    "print(f\"\\nModel Coefficients: {lr_model.coefficients}\")\n",
    "print(f\"Model Intercept: {lr_model.intercept}\")\n",
    "print(f\"RMSE: {lr_model.summary.rootMeanSquaredError}\")\n",
    "print(f\"R2: {lr_model.summary.r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Direct MongoDB Operations with PyMongo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymongo\n",
    "\n",
    "# Connect to MongoDB using PyMongo\n",
    "client = pymongo.MongoClient(\"mongodb://admin:admin@mongodb-dba:27017/?authSource=admin\")\n",
    "db = client.test\n",
    "collection = db.employees\n",
    "\n",
    "# Insert sample documents\n",
    "employees = [\n",
    "    {\"id\": 1, \"name\": \"John\", \"age\": 28, \"salary\": 50000, \"department\": \"Engineering\"},\n",
    "    {\"id\": 2, \"name\": \"Jane\", \"age\": 32, \"salary\": 75000, \"department\": \"Marketing\"},\n",
    "    {\"id\": 3, \"name\": \"Bob\", \"age\": 45, \"salary\": 95000, \"department\": \"Engineering\"},\n",
    "]\n",
    "\n",
    "# Clear existing data and insert new\n",
    "collection.delete_many({})\n",
    "collection.insert_many(employees)\n",
    "\n",
    "print(\"Data inserted into MongoDB!\")\n",
    "\n",
    "# Query data\n",
    "print(\"\\nAll employees from MongoDB:\")\n",
    "for employee in collection.find():\n",
    "    print(employee)\n",
    "\n",
    "# Query with filter\n",
    "print(\"\\nEngineering employees:\")\n",
    "for employee in collection.find({\"department\": \"Engineering\"}):\n",
    "    print(f\"{employee['name']} - ${employee['salary']}\")\n",
    "\n",
    "# Count documents\n",
    "print(f\"\\nTotal employees: {collection.count_documents({})}\")\n",
    "\n",
    "client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Convert PyMongo Data to Spark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymongo\n",
    "\n",
    "# Fetch data from MongoDB using PyMongo\n",
    "client = pymongo.MongoClient(\"mongodb://admin:admin@mongodb-dba:27017/?authSource=admin\")\n",
    "db = client.test\n",
    "collection = db.employees\n",
    "\n",
    "# Convert MongoDB cursor to list\n",
    "mongo_data = list(collection.find({}, {\"_id\": 0}))  # Exclude _id field\n",
    "\n",
    "# Create Spark DataFrame from MongoDB data\n",
    "if mongo_data:\n",
    "    df_from_pymongo = spark.createDataFrame(mongo_data)\n",
    "    print(\"MongoDB data as Spark DataFrame:\")\n",
    "    df_from_pymongo.show()\n",
    "    \n",
    "    print(\"\\nProcessing with Spark:\")\n",
    "    df_from_pymongo.groupBy(\"department\").avg(\"salary\").show()\n",
    "else:\n",
    "    print(\"No data in MongoDB collection\")\n",
    "\n",
    "client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Read from HDFS (if you have data in HDFS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Read CSV from HDFS\n",
    "# df_hdfs = spark.read.csv(\"hdfs://namenode:9000/data/your_file.csv\", header=True, inferSchema=True)\n",
    "# df_hdfs.show()\n",
    "\n",
    "# Example: Write to HDFS\n",
    "# df.write.mode(\"overwrite\").parquet(\"hdfs://namenode:9000/data/employees.parquet\")\n",
    "\n",
    "print(\"To read/write HDFS, uncomment the code above and provide the correct path\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Cleanup (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to stop Spark session when done\n",
    "# spark.stop()\n",
    "print(\"Keep Spark session running for further analysis.\")\n",
    "print(\"Uncomment spark.stop() above to stop the session.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygname": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
